# -*- coding: utf-8 -*-
"""mini-project-180482c (2).ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1HzkXAFWfTYu-HbTJPCoBPlYE3Lsu68ZL
"""
# Importing necessary packages

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
import numpy as np
import pandas as pd

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from xgboost import XGBClassifier
import lightgbm as lgb
import catboost as cb

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score

# Read input files

train_df = pd.read_parquet('/kaggle/input/american-express-data-parquet-format/train.parquet').groupby('customer_ID').tail(4)
test_df = pd.read_parquet('/kaggle/input/american-express-data-parquet-format/test.parquet').groupby('customer_ID').tail(4)
train_labels = pd.read_csv("../input/amex-default-prediction/train_labels.csv")
train_df.head()

# Feature engineering

# Categorical features
categorical_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']
print(len(categorical_columns))

# Numerical features
numeric_columns = [column for column in train_df.columns if column not in categorical_columns ]
print(len(numeric_columns))

# 1. Imputation

# Dropping columns with missing values greater than or equal to 75%
columns = train_df.columns[(train_df.isna().sum()/len(train_df))*100>75]
train_df = train_df.drop(columns, axis=1)
test_df = test_df.drop(columns, axis=1)

# Filling missing values
#       1. backfill - fill in missing values using the value from the next row
#       2. forward fill - fill in missing values using the value from the previous row

train_df = train_df.bfill(axis='rows').ffill(axis='rows')
test_df = test_df.bfill(axis='rows').ffill(axis='rows')

# Reset the index and drop the old index as a column from the dataframe

train_df.reset_index(inplace=True)
test_df.reset_index(inplace=True)

train_df =train_df.groupby('customer_ID').tail(1)
test_df = test_df.groupby('customer_ID').tail(1)

train_df.reset_index(inplace=True)
test_df.reset_index(inplace=True)


# 2 . Scaling

scaler = StandardScaler()
train_X = scaler.fit_transform(train_X)
test_X = scaler.transform(test_X)

# 3 . Categorical encoding

ordinal_encoder = OrdinalEncoder()
categorical_columns.remove('D_66')

train_df[categorical_columns] = ordinal_encoder.fit_transform(train_df[categorical_columns])
test_df[categorical_columns] = ordinal_encoder.transform(test_df[categorical_columns])

train_df = train_df.merge(train_labels, how='inner', on="customer_ID")

test_data = test_df.copy()
train_df = train_df.drop(['index','customer_ID', 'S_2'], axis=1)
test_df = test_df.drop(['index','customer_ID', 'S_2'], axis=1)

train_df = pd.get_dummies(train_df, columns=categorical_columns, drop_first=True)
test_df = pd.get_dummies(test_df, columns=categorical_columns, drop_first=True)

X = train_df.drop('target', axis=1)
y = train_df['target']

# Train Test Split

train_X, test_X , train_y , test_y = train_test_split(X,y,test_size=0.25,random_state=42)


# Modeling machine learning approaches

# 1. XGBoost

XGB = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.1).fit(train_X, train_y)

pred_y_xgb = XGB.predict(test_X)

test_data['prediction']=XGB.predict_proba(test_df)[:,1]
test_data[['customer_ID','prediction']].to_csv("XGB_submission_180482C.csv", index=False)

# 2. LightGBM

params = {
    "objective": "binary",
    "metric": "binary_logloss",
    "boosting_type": "gbdt",
    "num_leaves": 31,
    "learning_rate": 0.05,
    "feature_fraction": 0.9,
    "bagging_fraction": 0.8,
    "bagging_freq": 5,
    "verbose": 0
}

GBM = lgb.LGBMClassifier(**params)

GBM.fit(train_X, train_y, eval_set=[(test_X, test_y)])

pred_y_gbm = GBM.predict(test_X)

test_data['prediction']=GBM.predict_proba(test_df)[:,1]
test_data[['customer_ID','prediction']].to_csv("GBM_submission_180482C.csv", index=False)

# 3. CatBoost

CB = cb.CatBoostClassifier()

CB.fit(train_X, train_y, eval_set=(test_X, test_y))

pred_y_cb = CB.predict(test_X)

test_data['prediction']=CB.predict_proba(test_df)[:,1]
test_data[['customer_ID','prediction']].to_csv("CB_submission_180482C.csv", index=False)

# 4. SVM

SVM = svm.SVC(kernel='linear', C=1)
SVM.fit(train_X, train_y)

pred_y_svm = SVM.predict(test_X)

test_data['prediction']=SVM.predict_proba(test_df)[:,1]
test_data[['customer_ID','prediction']].to_csv("SVM_submission_180482C.csv", index=False)

# 5. KNN

KNN = KNeighborsClassifier(n_neighbors = 3)
KNN.fit(train_X,train_y)

pred_y_knn = KNN.predict(test_X)

test_data['prediction']=KNN.predict_proba(test_df)[:,1]
test_data[['customer_ID','prediction']].to_csv("KNN_submission_180482C.csv", index=False)

# Model evaluation

def calculate_metric_values(test_y, pred_y):
    mae = mean_absolute_error(test_y, pred_y)
    mse = mean_squared_error(test_y, pred_y)
    mape = mean_absolute_percentage_error(test_y, pred_y)
    r2 = r2_score(test_y, pred_y)
    ascore = accuracy_score(test_y, pred_y)
    
    return [mae, mse, mape, r2, ascore]

[mae_1, mse_1, mape_1, r2_1, ascore_1] = calculate_metric_values(test_y, pred_y_xgb)
[mae_2, mse_2, mape_2, r2_2, ascore_2] = calculate_metric_values(test_y, pred_y_gbm)
[mae_3, mse_3, mape_3, r2_3, ascore_3] = calculate_metric_values(test_y, pred_y_cb)
[mae_4, mse_4, mape_4, r2_4, ascore_4] = calculate_metric_values(test_y, pred_y_svm)
[mae_5, mse_5, mape_5, r2_5, ascore_5] = calculate_metric_values(test_y, pred_y_knn)

# Select best model

data = {'Approaches | Evaluation Metrics':['XGBoost', 'LightGBM', 'CatBoost', 'SVM', 'KNN'],'Mean Absolute Error':[mae_1 ,mae_2, mae_3 , '-', '-'], 'Mean Sqaured Error':[mse_1 ,mse_2, mse_3, '-', '-' ],'Mean Absolute Percentage Error':[mape_1 ,mape_2, mape_3, '-', '-' ], 'R2 Score':[r2_1 ,r2_2, r2_3, '-', '-' ], 'Accuracy Score':[ascore_1 ,ascore_2, ascore_3, '-', '-' ]}
df_1 = pd.DataFrame(data=data)
df_1